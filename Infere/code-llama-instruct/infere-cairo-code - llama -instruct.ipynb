{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in VSCODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if Torch supports CUDA\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hub_name = \"StarkWizard/codellama-cairo-instruct\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running this cell is mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba4dd9f9b624087ac9b623cf40a5fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Attention Sinks] Injected Position Shifting into 32 attention classes.\n",
      "[Attention Sinks] Injected Attention Sink KV Cache into 1 model class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map={\"\": 0},\n",
    "                                             attention_sink_size=4,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                            attention_sink_window_size=252, # <- Low for the sake of faster generation\n",
    "                                             )\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 16b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mattention_sinks\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     hub_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     attention_sink_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     attention_sink_window_size\u001b[39m=\u001b[39m\u001b[39m252\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mpad_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39meos_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub_name' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    hub_name,\n",
    "    load_in_4bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    \n",
    "    attention_sink_size=4,\n",
    "    attention_sink_window_size=252,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88b441ca09d42be910945c8f997304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Attention Sinks] Injected Position Shifting into 32 attention classes.\n",
      "[Attention Sinks] Injected Attention Sink KV Cache into 1 model class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreamer, GenerationConfig, BitsAndBytesConfig\n",
    "from attention_sinks import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    hub_name,\n",
    "    load_in_4bit=False,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    \n",
    "    attention_sink_size=4,\n",
    "    attention_sink_window_size=252,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from hub for inference\n",
    "\n",
    "- If you just need inference, run this\n",
    "- we load the model from HFace Hub in 4 bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Beam Search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pechaut/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [INST]\n",
      "<<SYS>>\n",
      "Write explanations and Cairo 1 code  to solve the following  problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
      "</SYS>>Question: I'm working in Cairo 1 :give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code \n",
      "[/INST]\n",
      "```\n",
      "#[starknet::interface]\n",
      "trait ISimpleStorage<TContractState> {\n",
      "    fn set_owner(ref self: TContractState, owner: felt252);\n",
      "    fn get_owner(self: @TContractState) -> felt252;\n",
      "}\n",
      "\n",
      "#[starknet::contract]\n",
      "mod SimpleStorage {\n",
      "\n",
      "    #[storage]\n",
      "    struct Storage {\n",
      "        owner: felt252,\n",
      "    }\n",
      "\n",
      "    #[external]\n",
      "    impl SimpleStorage of super::ISimpleStorage<ContractState> {\n",
      "        fn set_owner(ref self: ContractState, owner: felt252) {\n",
      "            self.owner.write(owner);\n",
      "        }\n",
      "\n",
      "        fn get_owner(self: @ContractState) -> felt252 {\n",
      "            self.owner.read()\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "#[starknet::interface]\n",
      "trait IFibonacci<TContractState> {\n",
      "    fn get_fibonacci(self: @TContractState) -> felt252;\n",
      "}\n",
      "\n",
      "#[starknet::contract]\n",
      "mod Fibonacci {\n",
      "\n",
      "    #[storage]\n",
      "    struct Storage {\n",
      "        owner: felt252,\n",
      "    }\n",
      "\n",
      "    #[external]\n",
      "    impl Fibonacci of super::IFibonacci<ContractState> {\n",
      "        fn get_fibonacci(self: @ContractState) -> felt252 {\n",
      "            let owner = self.owner.read();\n",
      "            if owner == 0 {\n",
      "                0\n",
      "            } else if owner == 1 {\n",
      "                1\n",
      "            } else {\n",
      "                get_fibonacci(owner - 1) + get_fibonacci(owner - 2)\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "#[starknet::interface]\n",
      "trait ISimpleStorage<TContractState> {\n",
      "    fn set(ref self: TContractState, x: felt252);\n",
      "    fn get(self: @TContractState) -> felt252;\n",
      "}\n",
      "\n",
      "#[starknet::contract]\n",
      "mod SimpleStorage {\n",
      "\n",
      "    #[storage]\n",
      "    struct Storage {\n",
      "        stored_data: felt25\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "\n",
    "    num_beams=2,\n",
    "    early_stopping=True,\n",
    "     output_attentions=False,\n",
    "      output_hidden_states=False,\n",
    "      temperature=0.001\n",
    "\n",
    ")\n",
    "prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "#prompt = \"give an exemple of a contract stores the value of the callers address,and provide a function to process the fibonnacci of that address\"\n",
    "#prompt = \"write an empty contract template\"\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following  problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "with torch.no_grad():\n",
    "    sequences = pipeline(\n",
    "    text,\n",
    "\n",
    "        num_return_sequences=1,\n",
    "                bos_token_id=model.config.bos_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.eos_token_id,\n",
    "\n",
    "    )\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Diverse beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgive an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m text \u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m[INST]\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m<<SYS>>\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mWrite explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m</SYS>>Question: I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm working in Cairo 1 :\u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m[/INST]\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m text,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m             bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m#max_length=600, # can increase the length of sequence\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResult: \u001b[39m\u001b[39m{\u001b[39;00mseq[\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1756\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1750\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1751\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1752\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1753\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1754\u001b[0m     )\n\u001b[1;32m   1755\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_beam_search(\n\u001b[1;32m   1757\u001b[0m         input_ids,\n\u001b[1;32m   1758\u001b[0m         beam_scorer,\n\u001b[1;32m   1759\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1760\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1761\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1762\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1763\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1764\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1765\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1766\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONSTRAINED_BEAM_SEARCH:\n\u001b[1;32m   1770\u001b[0m     final_constraints \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:3720\u001b[0m, in \u001b[0;36mGenerationMixin.group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m   3717\u001b[0m     batch_group_indices\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   3718\u001b[0m         [batch_idx \u001b[39m*\u001b[39m num_beams \u001b[39m+\u001b[39m idx \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(group_start_idx, group_end_idx)]\n\u001b[1;32m   3719\u001b[0m     )\n\u001b[0;32m-> 3720\u001b[0m group_input_ids \u001b[39m=\u001b[39m input_ids[batch_group_indices]\n\u001b[1;32m   3722\u001b[0m \u001b[39m# select outputs of beams of current group only\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[batch_group_indices, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    diversity_penalty=2.0,\n",
    "    early_stopping=True,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    max_new_tokens=512,\n",
    "      num_return_sequences=1,\n",
    "\n",
    ")\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "sequences = pipeline(\n",
    "text,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            pad_token_id=model.config.eos_token_id,\n",
    "    #max_length=600, # can increase the length of sequence\n",
    "\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Beam Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m text \u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m[INST]\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m<<SYS>>\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mWrite explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m</SYS>>Question: I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm working in Cairo 1 :\u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m[/INST]\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     text,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                 bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                 eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                 pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m, \u001b[39m# can increase the length of sequence\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResult: \u001b[39m\u001b[39m{\u001b[39;00mseq[\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1722\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1715\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1716\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1717\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1718\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1719\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m     \u001b[39m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1722\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_sample(\n\u001b[1;32m   1723\u001b[0m         input_ids,\n\u001b[1;32m   1724\u001b[0m         beam_scorer,\n\u001b[1;32m   1725\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1726\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1727\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1728\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1729\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1730\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1731\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1732\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1733\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1734\u001b[0m     )\n\u001b[1;32m   1736\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1737\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1739\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1740\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:3350\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3346\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3350\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3351\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3352\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3353\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3354\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3355\u001b[0m )\n\u001b[1;32m   3357\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3358\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/attention_sinks/inject_mixin.py:131\u001b[0m, in \u001b[0;36mInjectAttentionSinksMixin._inject_attention_sink_kv_cache.<locals>.overwrite_forward.<locals>.wrapped_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 131\u001b[0m     outputs \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    132\u001b[0m     outputs\u001b[39m.\u001b[39mpast_key_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_sink_kv_cache(outputs\u001b[39m.\u001b[39mpast_key_values)\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:632\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    630\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 632\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states)\n\u001b[1;32m    634\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    635\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    636\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    637\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    643\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:112\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    110\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    111\u001b[0m variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mrsqrt(variance \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariance_epsilon)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m hidden_states\u001b[39m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "\n",
    "    num_beams=3,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    temperature=0.9,\n",
    "    top_k=200,\n",
    "    top_p=1.5,\n",
    "\n",
    ")\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner'\n",
    "prompt =\"In cairo 1, increment i 6 times using  loop, provide an explanation of the code\"\n",
    "prompt = \"Create an array and append some domestic animal names and write a test to check the value of the item at index 2\"\n",
    "prompt = \" what is an array span ?\"\n",
    "prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write explanations and Cairo 1 code  to solve the following coding problem that obeys the constraints and passes the example test cases. Explain the code. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "</SYS>>Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "with torch.no_grad():\n",
    "    sequences = pipeline(\n",
    "    text,\n",
    "\n",
    "        num_return_sequences=1,\n",
    "                bos_token_id=model.config.bos_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.eos_token_id,\n",
    "        max_length=600, # can increase the length of sequence\n",
    "    )\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Contrastive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]\n",
      "<<SYS>>\n",
      "Write Cairo 1 code to solve the following coding problem that obeys the constraints and passes the example test cases. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
      "<</SYS>>\n",
      "Question: I'm working in Cairo 1 :give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 11.90 GiB of which 46.25 MiB is free. Including non-PyTorch memory, this process has 11.76 GiB memory in use. Of the allocated memory 11.38 GiB is allocated by PyTorch, and 230.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     streamer \u001b[39m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     generated_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mGenerationConfig(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             \u001b[39m# use_cache=True is required, the rest can be changed up.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m             min_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1050\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     penalty_alpha\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m             pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# Decode the final generated text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X31sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     output_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(generated_tokens[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1621\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mContrastive search requires `use_cache=True`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1623\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrastive_search(\n\u001b[1;32m   1624\u001b[0m         input_ids,\n\u001b[1;32m   1625\u001b[0m         top_k\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mtop_k,\n\u001b[1;32m   1626\u001b[0m         penalty_alpha\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpenalty_alpha,\n\u001b[1;32m   1627\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1628\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1629\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1630\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1631\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1632\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1633\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1634\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1635\u001b[0m         sequential\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mlow_memory,\n\u001b[1;32m   1636\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1637\u001b[0m     )\n\u001b[1;32m   1639\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mSAMPLE:\n\u001b[1;32m   1640\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:2007\u001b[0m, in \u001b[0;36mGenerationMixin.contrastive_search\u001b[0;34m(self, input_ids, top_k, penalty_alpha, logits_processor, logits_warper, stopping_criteria, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   2003\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2005\u001b[0m \u001b[39m# encode the given prefix and prepare model inputs; encoder-decoder model process the prefix and save\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m \u001b[39m# the `encoder_outputs`\u001b[39;00m\n\u001b[0;32m-> 2007\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2008\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m   2009\u001b[0m )\n\u001b[1;32m   2011\u001b[0m \u001b[39m# last decoder hidden states will be used to compute the degeneration penalty (cosine similarity with\u001b[39;00m\n\u001b[1;32m   2012\u001b[0m \u001b[39m# previous tokens)\u001b[39;00m\n\u001b[1;32m   2013\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/attention_sinks/inject_mixin.py:131\u001b[0m, in \u001b[0;36mInjectAttentionSinksMixin._inject_attention_sink_kv_cache.<locals>.overwrite_forward.<locals>.wrapped_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 131\u001b[0m     outputs \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    132\u001b[0m     outputs\u001b[39m.\u001b[39mpast_key_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_sink_kv_cache(outputs\u001b[39m.\u001b[39mpast_key_values)\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:649\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    647\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    648\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 649\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    650\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    652\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(down_proj)\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[1;32m    247\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 248\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul_4bit(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mt(), bias\u001b[39m=\u001b[39;49mbias, quant_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mquant_state)\n\u001b[1;32m    250\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[39mreturn\u001b[39;00m MatMul4Bit\u001b[39m.\u001b[39;49mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mempty(A\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m B_shape[:\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[39m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlinear(A, F\u001b[39m.\u001b[39;49mdequantize_4bit(B, state)\u001b[39m.\u001b[39mto(A\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[39m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m state\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/bitsandbytes/functional.py:908\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m absmax\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mfloat32: absmax \u001b[39m=\u001b[39m absmax\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    907\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 908\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mempty(shape, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mA\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    910\u001b[0m n \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    913\u001b[0m device \u001b[39m=\u001b[39m pre_call(A\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 11.90 GiB of which 46.25 MiB is free. Including non-PyTorch memory, this process has 11.76 GiB memory in use. Of the allocated memory 11.38 GiB is allocated by PyTorch, and 230.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig, StoppingCriteriaList,MaxLengthCriteria\n",
    "\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt = \"give an exemple of constructor\"\n",
    "#prompt=\"create an array 'exempl' that contains a u128, a u32, a u256\"\n",
    "prompt=\"provide a a contract with an external function that returns the fibonacci of the caller's address\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"create a function for fibonacci\"\n",
    "#prompt = \"what are spans used for\"\n",
    "#prompt = \"How do I know if an array is empty\"\n",
    "prompt =\"increment i 6 times using loop\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "#prompt = \"Create an array and append domestic animal names\"\n",
    "prompt = \"write a contract with a add function that returns the sum of 2 parameters\"\n",
    "#prompt = 'In Cairo 1, write a full sample contract with a constructor that stores the caller into owner''\n",
    "prompt = \"give an exemple of a  contract stores in owner the value of the callers address, add one function that returns the fibonacci value of owner, explain the code\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "Write Cairo 1 code to solve the following coding problem that obeys the constraints and passes the example test cases. Only use Cairo syntax and no other language. Please wrap your code answer using ```\n",
    "<</SYS>>\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "token_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = token_input['input_ids'].cuda()\n",
    "attention_mask = token_input['attention_mask'].cuda()\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=1)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache=True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "\n",
    "    penalty_alpha=0.6, \n",
    "    top_k=400,\n",
    "    attention_mask=attention_mask,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            pad_token_id=model.config.eos_token_id,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "write a contract for implementing a function that computes u(n)=u(n-1)+1, with n passed as parameter\n",
      "\n",
      "Answer:\n",
      "\n",
      "```\n",
      "    #[starknet::contract]\n",
      "    mod Function {\n",
      "\n",
      "        #[view]\n",
      "        fn compute_u(n: u128) -> u128 {\n",
      "            let mut result = 0_u128;\n",
      "            for i in 0_u128..n {\n",
      "                result += 1_u128;\n",
      "            }\n",
      "            result\n",
      "        }\n",
      "    }\n",
      "```\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#prompt = \"Create an array and append some animal names\"\n",
    "#prompt=\"create an array 'messages' that contains a u128, a u32, a u256\"\n",
    "#prompt = \"create a structure for mailAccount\"\n",
    "#prompt = \"create an array of felt and append 1 to the array\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"create a function for fibonacci\"\n",
    "#prompt = \"what are spans used for\"\n",
    "prompt = \"How do I know if an array is empty\"\n",
    "#prompt = \"what are spans used for\"\n",
    "prompt = \"write a contract for implementing a function that computes u(n)=u(n-1)+1, with n passed as parameter\"\n",
    "\n",
    "\n",
    "text =f\"[INST]I'm working in Cairo. You are a cairo expert and know no other computer language, answer in less than 170 words{prompt} [/INST]\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids,       \n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Answer:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(text):]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "// Stores the caller's address in a variable\n",
      "struct Storage {\n",
      "   owner: ContractAddress,\n",
      "}\n",
      "impl StorageImpl of super::Storage {\n",
      "   fn constructor(ref self: ContractState) {\n",
      "       self.owner.write(get_caller_address());\n",
      "   }\n",
      "}\n",
      "\n",
      "// Returns the Fibonacci of the stored address\n",
      "fn get_fibonacci() -> u64 {\n",
      "   0 // TODO return the Fibonacci of the stored address here\n",
      "}\n",
      "\n",
      "// This is the interface of your contract, it describes the functions this contract exposes to the outside world\n",
      "trait IContract {\n",
      "   fn get_fibonacci(self: @ContractState) -> u64;\n",
      "}\n",
      "\n",
      "// This is the implementation of the contract, it provides the actual behavior of the contract\n",
      "impl ContractImpl of super::IContract {\n",
      "   fn get_fibonnaci(self: @ContractState) -> u32 {\n",
      "       let owner = self.owner.read();\n",
      "       let mut result = 0;\n",
      "       for i in 0..owner {\n",
      "           result += i;\n",
      "       }\n",
      "       result\n",
      "   }\n",
      "}\n",
      "[/INST]\n",
      "```\n",
      "#[starknet::interface]\n",
      "trait IContract<TContractState> {\n",
      "   fn get_fibonaccy(self: TContractState) -> u64;\n",
      "   }\n",
      "\n",
      "#[starknet::contract]\n",
      "mod Fibonacci {\n",
      "   #[storage]\n",
      "   struct Storage {\n",
      "       owner: starknet::Address,\n",
      "   }\n",
      "\n",
      "   #[external]\n",
      "   impl Fibonacci of super::IContract<ContractState> {\n",
      "       fn get_fibonaccy(ref self: ContractState) -> u64 {\n",
      "           let owner = self.owner.read(); //TODO: implement logic here\n",
      "           let mut result = 0;\n",
      "           for i in 0..owner {\n",
      "               result += i;\n",
      "           }\n",
      "           result\n",
      "       }\n",
      "   }\n",
      "}\n",
      "```\n",
      "\n",
      "There you go! You have just written your first StarkNet contract.\n",
      "\n",
      "The contract has two parts: storage and external functions. The storage defines the variables that will be stored in the contracts storage. In this case, we store the owners address.\n",
      "The external functions define the actual logic of the contract. In this case, we calculate the result of the Fibonacci sequence based on the owners address.\n",
      "\n",
      "To run the tests, execute the command `scarb test --rpc http://0.0.0.0:5050` (make sure the RPC endpoint is correct for your local node).\n",
      "\n",
      "Congratulations! You have successfully written, compiled, and tested your first StarkNet contract using Scarb!\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code - llama -instruct.ipynb Cell 27\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     streamer \u001b[39m=\u001b[39m TextStreamer(tokenizer, skip_prompt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     generated_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mGenerationConfig(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m             \u001b[39m# use_cache=True is required, the rest can be changed up.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m             use_cache \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m             min_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m             max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1050\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m             penalty_alpha\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m             top_k\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m             do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m             top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m             no_repeat_ngram_size \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m             repetition_penalty \u001b[39m=\u001b[39;49m\u001b[39m1.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m             temperature\u001b[39m=\u001b[39;49m\u001b[39m0.00100\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m             bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m             eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m             \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m             pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m         ),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39m# Decode the final generated text\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/pechaut/prj/cairo-llm/Infere/code-llama-instruct/infere-cairo-code%20-%20llama%20-instruct.ipynb#X34sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     output_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(generated_tokens[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/utils.py:2747\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2744\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m   2746\u001b[0m \u001b[39m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 2747\u001b[0m next_token_scores \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[1;32m   2748\u001b[0m next_token_scores \u001b[39m=\u001b[39m logits_warper(input_ids, next_token_scores)\n\u001b[1;32m   2750\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/logits_process.py:756\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    754\u001b[0m num_batch_hypotheses \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    755\u001b[0m cur_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 756\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[1;32m    757\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[1;32m    758\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/logits_process.py:698\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ngram_size:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[39mreturn\u001b[39;00m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[0;32m--> 698\u001b[0m generated_ngrams \u001b[39m=\u001b[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001b[1;32m    699\u001b[0m banned_tokens \u001b[39m=\u001b[39m [\n\u001b[1;32m    700\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[1;32m    701\u001b[0m     \u001b[39mfor\u001b[39;00m hypo_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)\n\u001b[1;32m    702\u001b[0m ]\n\u001b[1;32m    703\u001b[0m \u001b[39mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/cairo-llm/lib/python3.9/site-packages/transformers/generation/logits_process.py:659\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[0;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[1;32m    657\u001b[0m generated_ngrams \u001b[39m=\u001b[39m [{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[1;32m    658\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos):\n\u001b[0;32m--> 659\u001b[0m     gen_tokens \u001b[39m=\u001b[39m prev_input_ids[idx]\u001b[39m.\u001b[39;49mtolist()\n\u001b[1;32m    660\u001b[0m     generated_ngram \u001b[39m=\u001b[39m generated_ngrams[idx]\n\u001b[1;32m    661\u001b[0m     \u001b[39m# Loop through each n-gram of size ngram_size in the list of tokens (gen_tokens)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "\n",
    "prompt = \"Create an array and append domestic animal names\"\n",
    "#prompt = \"give an exemple of a simplestorage contract with unit tests\"\n",
    "#prompt = \"create a felt and affect it a value of 1\"\n",
    "#prompt=\"In the example 'let y = { let x = 3; x + 1 };', what is the value of y? give a similar sample\"\n",
    "#prompt=\"How can you print a variable's value in Cairo? Give a full sample using an array\"\n",
    "#prompt = \"hat type of operations can be considered expressions in Cairo? Give an exemple.\"\n",
    "#prompt = \"What is a characteristic feature of Felt252Dict<T> when interacting with it? Give a sample\"\n",
    "#prompt = \"How do I create a span ? \"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "#prompt = \"what makes Cairo special\"\n",
    "#prompt = \"Create an array and append some domestic animal names\"\n",
    "#prompt=\"write a contract that computes the fibonacci of caller's address and explain the weakness of the program\"\n",
    "#prompt=\"write a contract that returns the fibonacci of a value passed as parameter. implement the test function for the value 10\"\n",
    "#prompt=\"write an empty contract template and give the instructions to compile and deploy it\"\n",
    "prompt=\"write a contract that stores the caller's address in its constructor and a function that returns the fibonnacci of that address\"\n",
    "#prompt=\"How can you implement the `Copy` trait on a custom type in Cairo?\"\n",
    "#prompt = \"What's the significance of the `self` parameter in my method of the `CubeGeom` trait?\"\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt=\"how can i cast a variable into another ?\"\n",
    "\n",
    "#prompt = \"What do the terms 'parent' and 'child' mean in the context of modules?\"\n",
    "#prompt=\"What can be defined in trait and impl blocks related to a type?\"\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples\"\n",
    "\n",
    "#prompt=\"write a  contract that returns the fibonacci of the caller's address\"\n",
    "#prompt = \"Create an array and append some domestic animal names as string\"\n",
    "\n",
    "\n",
    "\n",
    "#prompt = \"How do I know if an array is empty, give 2 examples \"\n",
    "\n",
    "#prompt = \"Provide a basic code snippet that defines a struct named 'User' with fields like 'active', 'username', and so on\"\n",
    "\n",
    "\n",
    "#prompt=\"write a contract template and give the instructions to compile and deploy it\"\n",
    "\n",
    "#prompt = \"write a contract with a function that returns the address of the caller\"\n",
    "#prompt = \"write a contract with a 'add_felt' function that gets the sum of 2 felt parameters\"\n",
    "#prompt = \"Create an array and append 3 domestic animal names as string and print the the second element\"\n",
    "#prompt = \"write a contract that returns the fibonacci of the caller address\"\n",
    "\n",
    "#prompt=\"write a  contract  that takes two arrays as parameter and return an array composed of the sum of the element of the arrays, write the full code\"\n",
    "#prompt = \"What's the difference between Nullable<T> and Option in terms of where the wrapped value is stored?\"\n",
    "\n",
    "\n",
    "#prompt = \"How does the Cairo convention style name functions and variables?\"\n",
    "\n",
    "#prompt = \"write a  contract that returns the fibonacci(caller address)\"\n",
    "\n",
    "#prompt = \"What does the #[starknet::component] attribute signify?\"\n",
    "\n",
    "#prompt = \" What is the naming convention for enum variants, give some examples\"\n",
    "\n",
    "#prompt =\"\"\"#[test]\n",
    "#[available_gas(200000)]\n",
    "#fn test_loop() {\n",
    "#    let mut counter = 0;\n",
    "#    //TODO make the test pass without changing any existing line\n",
    "#    loop {\n",
    "#        counter += 1;\n",
    "#    };\n",
    "#    assert(counter == 140, 'counter should be 140')\n",
    "#}\n",
    "#\"\"\"\n",
    "\n",
    "#prompt = \"write a  full contract that returns u(n)=u(n-1)+1\"\n",
    "\n",
    "text =f\"\"\"[INST]\n",
    "<<SYS>>\n",
    "A student asks you a question about Cairo 1. Provide a concise answer to the student's questions,do not expand the subject of the question, do not introduce any new topics or new question not provided by the student.\n",
    "Make sure the explanations never be longer than 300 words.Dont justify your answers. Dont give information not mentioned in the CONTEXT INFORMATION.provide only one solution <SYS>>\n",
    "\n",
    "Question: I'm working in Cairo 1 :{prompt} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "text =f\"[INST]You are a cairo 1 expert and know no other computer language, provide one short answer only{prompt} [/INST]\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            # use_cache=True is required, the rest can be changed up.\n",
    "            use_cache = True,\n",
    "            min_new_tokens=1,\n",
    "            max_new_tokens=1050,\n",
    "            penalty_alpha=0.1,\n",
    "            top_k=1000,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            no_repeat_ngram_size = 10,\n",
    "        \n",
    "            repetition_penalty =1.1,\n",
    "            temperature=0.00100,\n",
    "\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            \n",
    "            pad_token_id=model.config.eos_token_id,\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    # Decode the final generated text\n",
    "    output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
