{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0663c6b513a467ea3fed8428416a536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b57f557349949daad79c6a71971a538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00014.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add54bb5589a4f01a48cc50ad5643c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353d69014b784cdda806682fea0cdc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a8de2603dd4e2692fc9294c9e90534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d949b20d33cb46b387c7309ca6fd8a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00014.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c1b240591942deab178df3fd26685c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66c0389c5dd44e2ad5f772c97fadb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8605babd82d495f9bdcd94a43c1a7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80851fe5933d44b08105948e70bc5113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2651e5b164e24b6a95245e9a6c0de166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00014.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2d1d8ead5943438e4a27ec730fdc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8c267261f4447196043ff5cf2cfae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5491b7dfa5b42d591ad92ddb66ef0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b1d45d4c1b4fc6808f45c0ace1b01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00014.safetensors:   0%|          | 0.00/847M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc1ef6939a24280878cd4ec1ae64e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hub_name = \"StarkWizard/codellama-cairo-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=hub_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"cpu\",\n",
    "                                             offload_folder=\"offload\",\n",
    "                                             \n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 16318, done.\u001b[K\n",
      "remote: Counting objects: 100% (3383/3383), done.\u001b[K\n",
      "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
      "remote: Total 16318 (delta 3255), reused 3253 (delta 3175), pack-reused 12935\u001b[K\n",
      "Receiving objects: 100% (16318/16318), 18.86 MiB | 20.95 MiB/s, done.\n",
      "Resolving deltas: 100% (11350/11350), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pec/projets/cairo-llm-refacto/3-quantize/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/StarkWizard/codellama-cairo-instruct/resolve/main/tokenizer_config.json\n",
      "https://huggingface.co/StarkWizard/codellama-cairo-instruct/resolve/main/tokenizer.model\n",
      "https://huggingface.co/StarkWizard/codellama-cairo-instruct/resolve/main/tokenizer.json\n",
      "Error: 404\n",
      "https://huggingface.co/StarkWizard/codellama-cairo-instruct/resolve/main/special_tokens_map.json\n",
      "https://huggingface.co/StarkWizard/codellama-cairo-instruct/resolve/main/added_tokens.json\n",
      "Error: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def dl_HF(model_name,filename,save_path):\n",
    " url = f\"https://huggingface.co/{model_name}/resolve/main/{filename}\"\n",
    " print(url)\n",
    " r = requests.get(url, allow_redirects=True)\n",
    " if(r.status_code == 200):\n",
    "    open(f\"{save_path}{filename}\", 'wb').write(r.content)    \n",
    " else:\n",
    "    print(f\"Error: {r.status_code}\")\n",
    "\n",
    "files = [\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"tokenizer.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"added_tokens.json\",\n",
    "]\n",
    "\n",
    "save_path = \"./models/\"\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    dl_HF(hub_name,filename,save_path)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy~=1.24.4 (from -r ./requirements/requirements-convert.txt (line 1))\n",
      "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: sentencepiece~=0.1.98 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from -r ./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from -r ./requirements/requirements-convert.txt (line 3)) (4.36.2)\n",
      "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert.txt (line 4))\n",
      "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from -r ./requirements/requirements-convert.txt (line 5)) (4.25.2)\n",
      "Requirement already satisfied: torch~=2.1.1 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: filelock in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from jinja2->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/pec/anaconda3/envs/cairo-llm/lib/python3.11/site-packages (from sympy->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
      "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: numpy, gguf\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "Successfully installed gguf-0.6.0 numpy-1.24.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS:  \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/imatrix/imatrix.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o imatrix  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup  \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/passkey/passkey.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o passkey  \n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
     ]
    }
   ],
   "source": [
    "!make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pec/projets/cairo-llm-refacto/3-quantize/llama.cpp/gguf-py\n",
      "Loading model file models/model-00001-of-00003.safetensors\n",
      "Loading model file models/model-00001-of-00003.safetensors\n",
      "Loading model file models/model-00002-of-00003.safetensors\n",
      "Loading model file models/model-00003-of-00003.safetensors\n",
      "params = Params(n_vocab=32016, n_embd=4096, n_layer=32, n_ctx=16384, n_ff=11008, n_head=32, n_head_kv=32, f_norm_eps=1e-05, n_experts=None, n_experts_used=None, rope_scaling_type=None, f_rope_freq_base=1000000, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('models/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('models/tokenizer.json')}\n",
      "Loading vocab file 'models/tokenizer.model', type 'spm'\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32016, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32016, 4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "Writing models/ggml-model-f16.gguf, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to True\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32016 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   0\n",
      "[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   0\n",
      "[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 26/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 29/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   0\n",
      "[ 30/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 31/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 32/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 33/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 34/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 35/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 36/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 37/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 38/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 39/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 40/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 41/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 42/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 43/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 44/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 45/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 46/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 47/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 48/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 49/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 50/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 51/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 52/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 53/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 54/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 55/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 56/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 57/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 58/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 59/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 60/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 61/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 62/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 63/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 64/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
      "[ 65/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 66/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 67/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 68/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 69/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 70/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 71/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
      "[ 72/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 73/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 74/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 75/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 76/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 77/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 78/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 79/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 80/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 81/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 82/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 83/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 84/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 85/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 86/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 87/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 88/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 89/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 90/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 91/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[ 92/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 93/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 94/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 95/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 96/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 97/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 98/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2\n",
      "[ 99/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2\n",
      "[100/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2\n",
      "[101/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[102/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[103/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[104/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[105/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[106/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   2\n",
      "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   2\n",
      "[108/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   3\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   3\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[160/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[163/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5\n",
      "[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5\n",
      "[203/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5\n",
      "[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[205/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[208/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[209/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[210/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[211/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[213/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[214/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[215/291] Writing tensor output.weight                          | size  32016 x   4096  | type F16  | T+   6\n",
      "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[217/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[218/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+   8\n",
      "Wrote models/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert.py models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized model to models/codellama-cairo-instruct.Q8_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 1917 (57e2a7a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'models/ggml-model-f16.gguf' to 'models/codellama-cairo-instruct.Q8_0.gguf' as Q8_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from models/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = true\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 741600 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32016,     1,     1], type =    f16, quantizing to q8_0 .. size =   250.12 MiB ->   132.88 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.105 0.223 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.087 0.067 0.048 0.032 0.020 0.027 \n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.022 0.010 0.015 0.025 0.041 0.068 0.117 0.407 0.117 0.068 0.041 0.025 0.015 0.010 0.022 \n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.024 0.015 0.024 0.038 0.058 0.085 0.117 0.276 0.117 0.085 0.058 0.038 0.024 0.015 0.024 \n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.022 0.011 0.017 0.027 0.043 0.069 0.115 0.394 0.115 0.069 0.043 0.027 0.017 0.010 0.022 \n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.087 0.111 0.249 0.111 0.087 0.063 0.043 0.028 0.017 0.026 \n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.015 0.024 0.038 0.057 0.084 0.116 0.284 0.116 0.084 0.057 0.038 0.024 0.015 0.025 \n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.026 0.017 0.027 0.041 0.061 0.086 0.113 0.258 0.113 0.086 0.061 0.041 0.027 0.017 0.025 \n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.015 0.024 0.038 0.057 0.083 0.114 0.287 0.114 0.083 0.057 0.038 0.024 0.015 0.025 \n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.026 0.018 0.028 0.043 0.062 0.085 0.110 0.254 0.110 0.085 0.062 0.043 0.028 0.018 0.026 \n",
      "[  20/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  22/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  23/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[  24/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  26/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  27/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  28/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  29/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  30/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  31/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  32/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[  33/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[  34/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  35/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  36/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  37/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  38/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.238 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[  40/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  41/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.235 0.107 0.087 0.066 0.046 0.030 0.019 0.027 \n",
      "[  42/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.230 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
      "[  43/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  44/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  45/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  46/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  47/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.087 0.107 0.232 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
      "[  49/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  50/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  51/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.230 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
      "[  52/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  53/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  54/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  55/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  56/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.087 0.107 0.232 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
      "[  58/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  59/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.230 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
      "[  60/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[  61/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  62/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[  63/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  64/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  65/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.087 0.107 0.231 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
      "[  67/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  68/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  69/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[  70/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  71/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[  72/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  73/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[  74/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  76/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  77/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  78/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.230 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
      "[  79/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  80/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  81/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  82/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  83/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  85/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  86/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  87/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.229 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
      "[  88/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  89/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  90/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  91/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  92/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  94/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  95/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  96/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.229 0.106 0.087 0.066 0.047 0.031 0.020 0.027 \n",
      "[  97/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  98/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  99/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 100/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 101/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 103/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 104/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 105/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 106/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 107/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 108/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.229 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.087 0.106 0.228 0.106 0.087 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.229 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.087 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.087 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.087 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.087 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 209/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 210/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 213/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 214/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 215/ 291]                        output.weight - [ 4096, 32016,     1,     1], type =    f16, quantizing to q8_0 .. size =   250.12 MiB ->   132.88 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.087 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 217/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.109 0.237 0.109 0.088 0.065 0.045 0.030 0.019 0.027 \n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.032 0.020 0.027 \n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time =  8714.31 ms\n",
      "main:    total time =  8714.31 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.27 MB\n",
      "llama_model_quantize_internal: quant size  =  6828.77 MB\n",
      "llama_model_quantize_internal: hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['./quantize', 'models/ggml-model-f16.gguf', 'models/codellama-cairo-instruct.Q8_0.gguf', 'Q8_0'], returncode=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = hub_name.split('/')\n",
    "model_name_pure = parts[1]\n",
    "quant_type = \"Q8_0\"\n",
    "quantized_model_name = f\"models/{model_name_pure}.{quant_type}.gguf\"\n",
    "print(f\"Saving quantized model to {quantized_model_name}\")\n",
    "import subprocess\n",
    "\n",
    "command = [\"./quantize\",'models/ggml-model-f16.gguf',quantized_model_name,quant_type]\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbcd5ca252f4e64be6a19c7461981fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "codellama-cairo-instruct.Q8_0.gguf:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "quant_name = hub_name.split('/')[-1] + \"-GGUF\"\n",
    "repo_id = \"StarkWizard/\" + quant_name\n",
    "base_path=\"./models\"\n",
    "\n",
    "local_file_paths = [\n",
    "                    \n",
    "                    base_path + \"/model.safetensors.index.json\",\n",
    "                    base_path + \"/tokenizer_config.json\",\n",
    "                    base_path + \"/tokenizer.json\",\n",
    "                    base_path + \"/tokenizer.model\",\n",
    "                    base_path + \"/\"+f\"{model_name_pure}.{quant_type}.gguf\",\n",
    "                    base_path + \"/config.json\",\n",
    "                    base_path + \"/generation_config.json\",\n",
    "                    base_path + \"/special_tokens_map.json\",\n",
    "                    ]\n",
    "for l in local_file_paths:\n",
    "    file_name = l.split(\"/\")[-1]\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=l,\n",
    "        path_in_repo=file_name,\n",
    "        repo_id=repo_id,\n",
    "        repo_type = \"model\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
